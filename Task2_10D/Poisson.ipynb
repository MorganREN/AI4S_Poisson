{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following Poisson equation with Dirichlet boundary condition over a bounded domain $\\Omega\\subset [-1, 1]^2$\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "-\\Delta u(x) = f(x), & \\;\\text{in} \\;\\Omega,\\\\\n",
    "u(x) = g(x), & \\; \\text{on} \\; \\partial\\Omega, \n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "where $f, g$ are given functions. \n",
    "Denote $u(x;\\theta)$   the approximate NN solution with the set of parameters  $\\theta$  in the NN. The collection of all $u(x;\\theta)$ is used as the trial space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import *\n",
    "import time\n",
    "import copy\n",
    "import itertools\n",
    "from tqdm import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 10\n",
    "input_width, layer_width, output_width = dimension, 30, 1\n",
    "\n",
    "repeat = 3  # Editable\n",
    "epoch = 40000  # Editable\n",
    "lr = 1e-4  # Editable\n",
    "data_size = 2000  # Editable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# activation function\n",
    "def activation(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "# %%\n",
    "# build ResNet with one blocks\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self,input_width,layer_width):\n",
    "        super(Net,self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(input_width, layer_width)\n",
    "        self.layer2 = torch.nn.Linear(layer_width, layer_width)\n",
    "        self.layer3 = torch.nn.Linear(layer_width, layer_width)\n",
    "        self.layer4 = torch.nn.Linear(layer_width, 1)\n",
    "        self.initialize_weights()\n",
    "    def forward(self,x):\n",
    "        y = activation(self.layer1(x))\n",
    "        y = activation(self.layer3(activation(self.layer2(y)))) # residual block 1\n",
    "        output = self.layer4(y)\n",
    "        return output\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "    def print_network(self):\n",
    "        num_params = 0\n",
    "        for param in self.parameters():\n",
    "            num_params += param.numel()\n",
    "        # print(self)\n",
    "        print('Total number of parameters: %d' % num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Define the $f(x)$\n",
    "$$[-1,1]^2{\\ni }{\\vec x} {↦} f({\\vec x}) = -160π^2{∏}_{i=1}^2\\sin(4πx_i){∈}{\\mathbb R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, dimension):\n",
    "    temp = 1.0\n",
    "    for i in range(dimension):\n",
    "        temp = temp * torch.sin(4*pi*x[:, i])\n",
    "    u_temp = 1.0 * temp\n",
    "    f_temp = -160 * pi**2 * u_temp\n",
    "    return f_temp.reshape([x.size()[0], 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Generate sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(data_size, dimension):\n",
    "    sample_temp = torch.rand(data_size, dimension)\n",
    "    sample = sample_temp * 2 - 1\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Build the Model\n",
    "\n",
    "\\begin{equation}\n",
    "model = u(x;\\theta) =  \\prod_{i = 1}^{d} (1-x_i) (1 + x_i) \\cdot NN(x;\\theta).\n",
    "\\end{equation}\n",
    "where $NN(x;\\theta)$ is a function represented by a neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    x_temp = x\n",
    "    D_x_0 = torch.prod(1.0 - x_temp, axis=1).reshape([x.size()[0], 1])\n",
    "    D_x_1 = torch.prod(1.0 + x_temp, axis=1).reshape([x.size()[0], 1])\n",
    "    # scaling_factor = (1/5)**x.size()[1] # added scaling factor\n",
    "    # model_u_temp = (1/scaling_factor) * D_x_0 * D_x_1 * net(x)\n",
    "    model_u_temp = D_x_0 * D_x_1 * net(x)\n",
    "    return model_u_temp.reshape([x.size()[0], 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_residual(x, dimension):\n",
    "    u_hat = model(x)\n",
    "    grad_u_hat = torch.autograd.grad(outputs=u_hat, inputs=x, grad_outputs=torch.ones(u_hat.shape), create_graph=True)\n",
    "    laplace_u = torch.zeros([len(grad_u_hat[0]), 1])\n",
    "    for index in range(dimension):\n",
    "        p_temp = grad_u_hat[0][:, index].reshape([len(grad_u_hat[0]), 1])\n",
    "        temp = torch.autograd.grad(outputs=p_temp, inputs=x, grad_outputs=torch.ones(p_temp.shape), create_graph=True, allow_unused = True)[0]  # dxx\n",
    "        laplace_u = temp[:, index].reshape([len(grad_u_hat[0]), 1]) + laplace_u\n",
    "        loss_residual = torch.sum((-laplace_u - f(x, dimension))**2) / len(x)\n",
    "    return loss_residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Doing the Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Generating training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch = 0,loss = 2042.5200952414423 \n",
      " epoch = 500,loss = 1512.3675409675334 \n",
      " epoch = 1000,loss = 1210.90536781198 \n",
      " epoch = 1500,loss = 1132.7961493201992 \n",
      " epoch = 2000,loss = 1081.505019086143 \n",
      " epoch = 2500,loss = 1046.716429496614 \n",
      " epoch = 3000,loss = 1019.9476443683778 \n",
      " epoch = 3500,loss = 998.5946316089345 \n",
      " epoch = 4000,loss = 980.3202092509498 \n",
      " epoch = 4500,loss = 965.0173213400028 \n",
      " epoch = 5000,loss = 959.1842076402602 \n",
      " epoch = 5500,loss = 939.1678214253988 \n",
      " epoch = 6000,loss = 928.6173798909794 \n",
      " epoch = 6500,loss = 929.3141745427142 \n",
      " epoch = 7000,loss = 908.3430237290897 \n",
      " epoch = 7500,loss = 911.0069850149148 \n",
      " epoch = 8000,loss = 890.5391534411714 \n",
      " epoch = 8500,loss = 881.7516532668312 \n",
      " epoch = 9000,loss = 873.6138706551707 \n",
      " epoch = 9500,loss = 863.9409181583364 \n",
      " epoch = 10000,loss = 855.9360864878006 \n",
      " epoch = 10500,loss = 855.0909484389969 \n",
      " epoch = 11000,loss = 837.7737848415461 \n",
      " epoch = 11500,loss = 835.9376270791704 \n",
      " epoch = 12000,loss = 820.387769001418 \n",
      " epoch = 12500,loss = 812.9532941146265 \n",
      " epoch = 13000,loss = 807.8001110376115 \n",
      " epoch = 13500,loss = 834.7784960651418 \n",
      " epoch = 14000,loss = 787.1660277734809 \n",
      " epoch = 14500,loss = 778.488323355875 \n",
      " epoch = 15000,loss = 773.2141110214993 \n",
      " epoch = 15500,loss = 760.7798121420886 \n",
      " epoch = 16000,loss = 751.7995202376777 \n",
      " epoch = 16500,loss = 744.7431146238802 \n",
      " epoch = 17000,loss = 738.1004372074556 \n",
      " epoch = 17500,loss = 727.2555702025418 \n",
      " epoch = 18000,loss = 718.7911093626759 \n",
      " epoch = 18500,loss = 720.9166883440613 \n",
      " epoch = 19000,loss = 703.9911697870192 \n",
      " epoch = 19500,loss = 696.1988301117973 \n",
      " epoch = 20000,loss = 689.1831260438653 \n",
      " epoch = 20500,loss = 684.2028387714122 \n",
      " epoch = 21000,loss = 683.126107209454 \n",
      " epoch = 21500,loss = 670.7128342264879 \n",
      " epoch = 22000,loss = 663.0072956878291 \n",
      " epoch = 22500,loss = 656.5449087052622 \n",
      " epoch = 23000,loss = 652.1294924961168 \n",
      " epoch = 23500,loss = 663.5654352978703 \n",
      " epoch = 24000,loss = 640.7989333696736 \n",
      " epoch = 24500,loss = 636.4985873346882 \n",
      " epoch = 25000,loss = 632.0760397315212 \n",
      " epoch = 25500,loss = 629.3975098651282 \n",
      " epoch = 26000,loss = 629.4159021307798 \n",
      " epoch = 26500,loss = 633.8056611215252 \n",
      " epoch = 27000,loss = 627.0331932826086 \n",
      " epoch = 27500,loss = 613.103339199709 \n",
      " epoch = 28000,loss = 619.0985285369015 \n",
      " epoch = 28500,loss = 608.3995495740904 \n",
      " epoch = 29000,loss = 607.7493289410857 \n",
      " epoch = 29500,loss = 608.8217525239596 \n",
      " epoch = 30000,loss = 612.8198595384977 \n",
      " epoch = 30500,loss = 600.1752783084775 \n",
      " epoch = 31000,loss = 594.2475199430296 \n",
      " epoch = 31500,loss = 590.5395100312694 \n",
      " epoch = 32000,loss = 588.1288222900405 \n",
      " epoch = 32500,loss = 621.9450440216685 \n",
      " epoch = 33000,loss = 587.5604037422748 \n",
      " epoch = 33500,loss = 593.4439676718181 \n",
      " epoch = 34000,loss = 587.9621463253189 \n",
      " epoch = 34500,loss = 579.3667117914921 \n",
      " epoch = 35000,loss = 577.1991395112609 \n",
      " epoch = 35500,loss = 580.6079695406711 \n",
      " epoch = 36000,loss = 579.6125718162424 \n",
      " epoch = 36500,loss = 584.2072019173205 \n",
      " epoch = 37000,loss = 596.1438278289838 \n",
      " epoch = 37500,loss = 572.4506758553434 \n",
      " epoch = 38000,loss = 570.948233766772 \n",
      " epoch = 38500,loss = 571.7294120379465 \n",
      " epoch = 39000,loss = 569.9865910258997 \n",
      " epoch = 39500,loss = 564.6265918407083 \n"
     ]
    }
   ],
   "source": [
    "net = Net(input_width, layer_width)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_epoch = []\n",
    "x = generate_sample(data_size, input_width)\n",
    "x.requires_grad = True\n",
    "for i in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function_residual(x, input_width)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    torch.cuda.empty_cache()\n",
    "    if i % 500 == 0:\n",
    "        print(\" epoch = {},loss = {} \".format(i, loss))\n",
    "    loss_epoch.append(float(loss))\n",
    "    # error_epoch.append(float(test_sample_fractional_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ploting the Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "test_data = torch.zeros((101, 101, 2))\n",
    "for i in range(101):\n",
    "    for j in range(101):\n",
    "        test_data[i, j, :] = torch.tensor([i/50-1, j/50-1])\n",
    "test_data = test_data.reshape(101*101, 2)\n",
    "\n",
    "# u_hat = model(test_data)\n",
    "# u_hat = u_hat.reshape(101, 101)\n",
    "# u_hat = u_hat.detach().numpy()\n",
    "\n",
    "# save model\n",
    "torch.save(net.state_dict(), '10D-Poisson.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
